{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      ">> current directory : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\n",
      ">> cache path : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\\.cache\n",
      ">> model path : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\\.cache\\models\n",
      ">> dataset path : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\\.cache\\dataset\n",
      ">> logs path : C:\\Users\\loicf\\Documents\\IRISA\\ExplanationPairSentencesTasks\\.cache\\logs\n",
      ">> device : cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "cwd = os.getcwd().split(os.path.sep)\n",
    "\n",
    "# point to the git repository\n",
    "while cwd[-1] != \"ExplanationPairSentencesTasks\":\n",
    "    os.chdir(\"..\")\n",
    "    cwd = os.getcwd().split(os.path.sep)\n",
    "print(f\">> current directory : {os.getcwd()}\")\n",
    "\n",
    "# add the root directory\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "\n",
    "# cache and data cache\n",
    "cache_path = path.join(os.getcwd() ,'.cache')\n",
    "dataset_path = path.join(cache_path, 'dataset')\n",
    "log_path = path.join(cache_path, 'logs')\n",
    "model_path = path.join(cache_path, 'models')\n",
    "print(f\">> cache path : {cache_path}\")\n",
    "print(f\">> model path : {model_path}\")\n",
    "print(f\">> dataset path : {dataset_path}\")\n",
    "print(f\">> logs path : {log_path}\")\n",
    "\n",
    "# Data Modules\n",
    "from src.data_module.hatexplain import CLSTokenHateXPlainDM\n",
    "from src.data_module.esnli import CLSTokenESNLIDM\n",
    "from src.data_module.yelp_hat import CLSTokenYelpHat50DM, CLSTokenYelpHatDM\n",
    "\n",
    "# Model\n",
    "from pur_attention_key_reg import AttitModel\n",
    "\n",
    "# Utils\n",
    "from notebooks.attention_based.utils.attention_rollout import rollout\n",
    "from notebooks.attention_based.utils.attention_embeddings import dict_print\n",
    "\n",
    "# External librairies\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\">> device : {DEVICE}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HateXplain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "proceed the cosine map: : 29it [00:00, 50.62it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed : torch.Size([21056])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "proceed the cosine map: : 29it [00:00, 69.74it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed : torch.Size([21056])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "proceed the cosine map: : 29it [00:00, 56.94it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed : torch.Size([21056])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "proceed the cosine map: : 29it [00:00, 48.66it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed : torch.Size([21056])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AveragePrecision` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\loicf\\miniconda3\\envs\\nlp\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "proceed the cosine map: : 29it [00:00, 45.20it/s]                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed : torch.Size([21056])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dm_kwargs = dict(cache_path=dataset_path,\n",
    "                 batch_size=32,\n",
    "                 num_workers=0,\n",
    "                 n_data=900\n",
    "                 )\n",
    "\n",
    "dm = CLSTokenHateXPlainDM(**dm_kwargs)\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"test\")\n",
    "test_dataloader = dm.test_dataloader()\n",
    "\n",
    "model_args = dict(\n",
    "        cache_path=model_path,\n",
    "        mode=\"exp\",\n",
    "        vocab=dm.vocab,\n",
    "        lambda_entropy=0,\n",
    "        lambda_supervise=0,\n",
    "        lambda_lagrange=0,\n",
    "        pretrained_vectors=\"glove.840B.300d\",\n",
    "        num_layers=1,\n",
    "        num_heads=1,\n",
    "        d_embedding=300,\n",
    "        data=\"hatexplain\",\n",
    "        num_class=dm.num_class,\n",
    "        opt=\"adam\"\n",
    ")\n",
    "\n",
    "res_array = [None, None, None, None, None]\n",
    "for l in range(5):\n",
    "    model_args[\"num_layers\"] = l+1\n",
    "    ckp = os.path.join(log_path, \"PurAttention\", f\"run=0_hatexplain_l=0{l+1}_h=1_adam\", \"checkpoints\", \"best.ckpt\")\n",
    "    hparams = os.path.join(log_path, \"PurAttention\", f\"run=0_hatexplain_l=0{l+1}_h=1_adam\", \"hparams.yaml\")\n",
    "\n",
    "    # the model\n",
    "    model = AttitModel.load_from_checkpoint(ckp, hparams_file=hparams, **model_args)\n",
    "    model = model.eval()\n",
    "\n",
    "    res = rollout(model.to(DEVICE), dm)\n",
    "    res_array[l] = res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of the rollout for the layer 0 : 0.584617025163806\n",
      "AUC of the rollout for the layer 1 : 0.5926617317105185\n",
      "AUC of the rollout for the layer 2 : 0.5951004462501168\n",
      "AUC of the rollout for the layer 3 : 0.5732684009178745\n",
      "AUC of the rollout for the layer 4 : 0.5866986012662915\n"
     ]
    }
   ],
   "source": [
    "for i,res in enumerate(res_array):\n",
    "    print(f\"AUC of the rollout for the layer {i} : \", end = \"\")\n",
    "    print(res[\"AUC - cos\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse calculated between layer 0 and rollout layer 1 : 0.72984844\n",
      "mse calculated between layer 0 and rollout layer 2 : 0.81446034\n",
      "mse calculated between layer 0 and rollout layer 3 : 0.6375018\n",
      "mse calculated between layer 0 and rollout layer 4 : 0.6733792\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(res_array):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    print(f\"mse calculated between layer 0 and rollout layer {i} : \", end=\"\")\n",
    "    print(np.mean((res[\"rollout_values\"] - res_array[0][\"rollout_values\"])**2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "il n'y a pas de réelle différence entre nos différents modèles. Ceci vient directement du comportement de l'attention dans ce genre de réseau quand on rajoute plusieurs couches.\n",
    "Ici le fait de rajouter des couches ne change absolument rien au comportement de notre modèle."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}